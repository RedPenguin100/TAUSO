{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "# Batch Processing with PFRED on WSL 2\n",
    "\n",
    "This guide covers setting up the PFRED backend on Windows (WSL 2) and running a batch analysis on your data without the GUI.\n",
    "\n",
    "## Part 1: Fix Docker Compatibility\n",
    "The PFRED container uses an older OS that causes \"Segmentation Faults\" (Exit Code 139) on modern WSL kernels. You must enable legacy emulation.\n",
    "\n",
    "1.  **Create Config File**:\n",
    "    * Go to `C:\\Users\\<your_username>\\` in Windows.\n",
    "    * Create a file named `.wslconfig` (no extension).\n",
    "    * Add this content:\n",
    "        ```ini\n",
    "        [wsl2]\n",
    "        kernelCommandLine = vsyscall=emulate\n",
    "        ```\n",
    "\n",
    "2.  **Restart WSL**:\n",
    "    * Open **PowerShell** (not WSL) and run:\n",
    "        ```powershell\n",
    "        wsl --shutdown\n",
    "        ```\n",
    "    * Wait 10 seconds, then reopen your WSL terminal.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Build and Start PFRED\n",
    "Since the docker image isn't public, you must build it locally.\n",
    "And since the original image is very old, I had to update the Dockerfile with my own fork.\n",
    "\n",
    "bash\n",
    "# 1. Clone the repo\n",
    "`git clone git@github.com:RedPenguin100/PFRED-fork.git pfred-docker`\n",
    "\n",
    "# 2. Build the image manually\n",
    "`docker build -t tauso/pfred:v1 ./pfred-docker`\n",
    "\n",
    "# 3. Start the service\n",
    "`docker run -d -t --name pfred tauso/pfred:v1`"
   ],
   "id": "e34d2bb58bae6c3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üõ†Ô∏è Critical Troubleshooting: Line Endings & Compatibility\n",
    "\n",
    "If the container crashes or reports a \"file not found\" error for `entrypoint.sh`, it is likely due to **Windows Line Endings (CRLF)** leaking into the scripts. Linux environments strictly require the **LF** format to execute shell scripts and read the Dockerfile correctly.\n",
    "\n",
    "Before running `docker build`, execute the following command in your WSL terminal to sanitize your files:\n",
    "\n",
    "```bash\n",
    "# Fix line endings for all configuration and script files\n",
    "dos2unix Dockerfile entrypoint.sh"
   ],
   "id": "613847013cfd7a98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "\n",
    "# We define a simple container name based on your image name\n",
    "CONTAINER_NAME = \"pfred\"\n",
    "IMAGE_TAG = \"tauso/pfred:v1\"\n",
    "\n",
    "print(f\"Checking status of container '{CONTAINER_NAME}' (Image: {IMAGE_TAG})...\")\n",
    "\n",
    "try:\n",
    "    # Check if the container is running\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"inspect\", \"-f\", \"{{.State.Running}}\", CONTAINER_NAME],\n",
    "        capture_output=True, text=True, check=True\n",
    "    )\n",
    "\n",
    "    if result.stdout.strip() == \"true\":\n",
    "        print(f\"‚úÖ [SUCCESS] The container '{CONTAINER_NAME}' is active and ready.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è [WARNING] Container '{CONTAINER_NAME}' exists but is STOPPED.\")\n",
    "        print(f\"To start it, run: docker start {CONTAINER_NAME}\")\n",
    "\n",
    "except subprocess.CalledProcessError:\n",
    "    print(f\"‚ùå [ERROR] Container '{CONTAINER_NAME}' not found.\")\n",
    "    print(f\"You need to create it first using your image:\")\n",
    "    print(f\"Run: docker run -d -t --name {CONTAINER_NAME} {IMAGE_TAG}\")"
   ],
   "id": "8be9cb4cebe3393b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T09:26:02.542594Z",
     "start_time": "2026-01-07T09:25:32.066563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from notebooks.consts import *\n",
    "\n",
    "all_data = pd.read_csv(str(UPDATED_CSV), low_memory=False)\n",
    "all_data[CELL_LINE_ORGANISM] = 'human'\n",
    "from notebooks.notebook_utils import log_correction, get_unique_human_genes\n",
    "\n",
    "# Create a new column with transformed inhibition values on a negative log scale\n",
    "log_correction(all_data, correction=5)  # to avoid log 0\n",
    "from notebooks.notebook_utils import read_cached_gene_to_data\n",
    "from tauso.new_model.data_handling import get_populated_df_with_structure_features\n",
    "\n",
    "genes_u = get_unique_human_genes(all_data)\n",
    "gene_to_data = read_cached_gene_to_data(genes_u)\n",
    "all_data_human_gene = get_populated_df_with_structure_features(all_data, genes_u, gene_to_data)\n",
    "all_data_human_gene.columns"
   ],
   "id": "342380f5db9e1ee0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'ISIS', 'Target_gene', 'Cell_line', 'Density(cells/well)',\n",
       "       'Transfection', 'ASO_volume(nM)', 'Treatment_Period(hours)',\n",
       "       'Primer_probe_set', 'Sequence', 'Modification', 'Location',\n",
       "       'Chemical_Pattern', 'Linkage', 'Linkage_Location', 'Smiles',\n",
       "       'Inhibition(%)', 'seq_length', 'Canonical Gene Name',\n",
       "       'Cell line organism', 'Transcript', 'Location_in_sequence',\n",
       "       'Location_div_by_length', 'true_length_of_seq', 'mod_scan',\n",
       "       'cell_line_uniform', 'log_inhibition', 'sense_start',\n",
       "       'sense_start_from_end', 'sense_length', 'sense_exon', 'sense_intron',\n",
       "       'sense_utr', 'sense_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T09:30:56.670658Z",
     "start_time": "2026-01-07T09:30:24.842232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from notebooks.consts import UPDATED_CSV, CANONICAL_GENE\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "CONTAINER_NAME = \"pfred\"\n",
    "TEMP_HOST_DIR = os.path.abspath(\"./pfred_temp_io\")\n",
    "os.makedirs(TEMP_HOST_DIR, exist_ok=True)\n",
    "OUTPUT_SCORED_CSV = \"data_scored_pfred_final.csv\"\n",
    "\n",
    "# --- CORE FUNCTION: BATCH SCORING ---\n",
    "def score_with_pfred_batch(input_df):\n",
    "    \"\"\"\n",
    "    Runs PFRED on a batch of unique sequences.\n",
    "    Input: DataFrame with ['name', 'seq']\n",
    "    Output: DataFrame with ['seq', 'SVMpred', 'PLSpred']\n",
    "    \"\"\"\n",
    "    if input_df.empty: return pd.DataFrame()\n",
    "\n",
    "    local_in = os.path.join(TEMP_HOST_DIR, \"pfred_input.csv\")\n",
    "    local_out = os.path.join(TEMP_HOST_DIR, \"pfred_results.csv\")\n",
    "    remote_dir = \"/home/pfred/scratch\"\n",
    "    remote_in = \"pfred_input.csv\"\n",
    "\n",
    "    # Cleanup previous run\n",
    "    if os.path.exists(local_out): os.remove(local_out)\n",
    "\n",
    "    # 1. Format for PFRED: [Seq, ID, DummyScore]\n",
    "    #    The dummy score (1.0) ensures the legacy script processes the line.\n",
    "    export_df = pd.DataFrame({\n",
    "        'col0': input_df['seq'],\n",
    "        'col1': input_df['name'],\n",
    "        'col2': 1.0\n",
    "    })\n",
    "    export_df.to_csv(local_in, index=False, header=[\"Seq\", \"ID\", \"Score\"])\n",
    "\n",
    "    try:\n",
    "        # 2. Transfer to Docker\n",
    "        subprocess.run([\"docker\", \"exec\", CONTAINER_NAME, \"mkdir\", \"-p\", remote_dir], check=False)\n",
    "        subprocess.run([\"docker\", \"cp\", local_in, f\"{CONTAINER_NAME}:{remote_dir}/{remote_in}\"], check=True)\n",
    "\n",
    "        # 3. Execute PFRED (Predict Mode)\n",
    "        #    Note: We pipe input_params.txt to handle the interactive prompts of the legacy script\n",
    "        cmd = f\"\"\"\n",
    "        cd {remote_dir}\n",
    "        tr -d '\\\\r' < {remote_in} > {remote_in}.tmp && mv {remote_in}.tmp {remote_in}\n",
    "        echo -e \"15\\\\n21\\\\n100\\\\n1000\\\\n12\" > input_params.txt\n",
    "\n",
    "        export PATH=/home/pfred/bin/R2.6.0/bin:$PATH\n",
    "        export R_HOME=/home/pfred/bin/R2.6.0/lib64/R\n",
    "        export PYTHONPATH=/usr/lib64/python2.6/site-packages:$PYTHONPATH\n",
    "\n",
    "        python2 /home/pfred/scripts/pfred/antisense_predictor.py \\\n",
    "            AOBase \\\n",
    "            /home/pfred/scripts/pfred/AOBase_542seq_cleaned_modelBuilding_Jan2009_15_21_noOutliers.csv \\\n",
    "            c_a_thermo \\\n",
    "            predict \\\n",
    "            ./{remote_in} \\\n",
    "            < input_params.txt > run.log 2>&1\n",
    "\n",
    "        ls -t *.csv | grep -v \"{remote_in}\" | head -n 1\n",
    "        \"\"\"\n",
    "\n",
    "        result = subprocess.run(\n",
    "            [\"docker\", \"exec\", CONTAINER_NAME, \"/bin/bash\", \"-c\", cmd],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "\n",
    "        target_file = result.stdout.strip()\n",
    "        if not target_file: return pd.DataFrame()\n",
    "\n",
    "        # 4. Retrieve Results\n",
    "        subprocess.run([\"docker\", \"cp\", f\"{CONTAINER_NAME}:{remote_dir}/{target_file}\", local_out], check=True)\n",
    "        # Cleanup remote to keep container light\n",
    "        subprocess.run([\"docker\", \"exec\", CONTAINER_NAME, \"rm\", \"-rf\", remote_dir], check=False)\n",
    "\n",
    "        if os.path.exists(local_out):\n",
    "            return pd.read_csv(local_out)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Docker execution failed: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ==========================================\n",
    "# MAIN EXECUTION\n",
    "# ==========================================\n",
    "print(\"\\n>>> STARTING PFRED ANNOTATION PIPELINE...\")\n",
    "\n",
    "if os.path.exists(UPDATED_CSV):\n",
    "    full_df = pd.read_csv(UPDATED_CSV, low_memory=False)\n",
    "    print(f\"   Loaded {len(full_df)} rows from '{UPDATED_CSV}'.\")\n",
    "\n",
    "    # 1. Standardize Sequences for Matching\n",
    "    #    We create a temporary 'clean_seq' column to handle U/T and non-ACTG chars.\n",
    "    print(\"   Standardizing sequences...\")\n",
    "    full_df['clean_seq'] = full_df['Sequence'].astype(str).str.upper().str.replace('U', 'T')\n",
    "    full_df['clean_seq'] = full_df['clean_seq'].apply(lambda x: re.sub(r'[^ACTG]', '', x))\n",
    "\n",
    "    # 2. Filter Safe Sequences\n",
    "    #    PFRED crashes if length <= Lag (approx 15). We only score safe ones.\n",
    "    safe_mask = full_df['clean_seq'].str.len() >= 16\n",
    "    print(f\"   Excluded {len(full_df) - safe_mask.sum()} sequences (<16bp) from scoring.\")\n",
    "\n",
    "    # 3. Extract Unique Sequences (Optimization)\n",
    "    #    Scoring 5k unique is faster than 30k rows with duplicates.\n",
    "    unique_seqs = full_df.loc[safe_mask, 'clean_seq'].unique()\n",
    "    print(f\"   Found {len(unique_seqs)} unique sequences to score.\")\n",
    "\n",
    "    # 4. Batch Scoring\n",
    "    CHUNK_SIZE = 5000 # Safe batch size for Docker piping\n",
    "    all_scores = []\n",
    "\n",
    "    for i in range(0, len(unique_seqs), CHUNK_SIZE):\n",
    "        batch = unique_seqs[i : i+CHUNK_SIZE]\n",
    "        print(f\"   ...Processing batch {i} to {i+len(batch)}...\")\n",
    "\n",
    "        batch_input = pd.DataFrame({\n",
    "            'name': [f\"s_{k}\" for k in range(len(batch))], # Dummy IDs\n",
    "            'seq': batch\n",
    "        })\n",
    "\n",
    "        batch_res = score_with_pfred_batch(batch_input)\n",
    "\n",
    "        if not batch_res.empty:\n",
    "            # Handle variable output column names from PFRED versions\n",
    "            if 'antisense_strand__5_3' in batch_res.columns:\n",
    "                 batch_res = batch_res.rename(columns={'antisense_strand__5_3': 'seq'})\n",
    "            elif 'Seq' in batch_res.columns:\n",
    "                 batch_res = batch_res.rename(columns={'Seq': 'seq'})\n",
    "\n",
    "            # Extract relevant columns\n",
    "            if 'SVMpred' in batch_res.columns:\n",
    "                all_scores.append(batch_res[['seq', 'SVMpred', 'PLSpred']])\n",
    "\n",
    "    # 5. Merge Scores Back to Main DataFrame\n",
    "    if all_scores:\n",
    "        print(\"   Merging scores back to main dataset...\")\n",
    "        score_df = pd.concat(all_scores, ignore_index=True)\n",
    "\n",
    "        # Create mapping dictionaries\n",
    "        # Note: We duplicate-drop score_df just in case, though unique_seqs handles it\n",
    "        score_df = score_df.drop_duplicates(subset=['seq'])\n",
    "\n",
    "        svm_map = dict(zip(score_df['seq'], score_df['SVMpred']))\n",
    "        pls_map = dict(zip(score_df['seq'], score_df['PLSpred']))\n",
    "\n",
    "        # Map values to the full dataframe using the 'clean_seq' key\n",
    "        full_df['PFRED_SVM'] = full_df['clean_seq'].map(svm_map)\n",
    "        full_df['PFRED_PLS'] = full_df['clean_seq'].map(pls_map)\n",
    "\n",
    "        # Clean up temporary column\n",
    "        full_df.drop(columns=['clean_seq'], inplace=True)\n",
    "\n",
    "        # Save Final Result\n",
    "        full_df.to_csv(OUTPUT_SCORED_CSV, index=False)\n",
    "        print(f\"\\n[SUCCESS] Final dataset with PFRED scores saved to: {OUTPUT_SCORED_CSV}\")\n",
    "\n",
    "        # Quick check\n",
    "        n_scored = full_df['PFRED_SVM'].notna().sum()\n",
    "        print(f\"   Total rows annotated: {n_scored} / {len(full_df)}\")\n",
    "    else:\n",
    "        print(\"[FAILURE] No valid scores returned from PFRED.\")\n",
    "\n",
    "else:\n",
    "    print(f\"[ERROR] Input file '{UPDATED_CSV}' not found.\")"
   ],
   "id": "1017ac53419d8edd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> STARTING PFRED ANNOTATION PIPELINE...\n",
      "   Loaded 34765 rows from '/home/michael/career/TAUSO/notebooks/data/data_asoptimizer_updated.csv'.\n",
      "   Standardizing sequences...\n",
      "   Excluded 173 sequences (<16bp) from scoring.\n",
      "   Found 15151 unique sequences to score.\n",
      "   ...Processing batch 0 to 5000...\n",
      "   ...Processing batch 5000 to 10000...\n",
      "   ...Processing batch 10000 to 15000...\n",
      "   ...Processing batch 15000 to 15151...\n",
      "   Merging scores back to main dataset...\n",
      "\n",
      "[SUCCESS] Final dataset with PFRED scores saved to: data_scored_pfred_final.csv\n",
      "   Total rows annotated: 34592 / 34765\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from notebooks.features.feature_extraction import save_feature\n",
    "\n",
    "if False:\n",
    "    save_feature(full_df, 'PFRED_PLS')\n",
    "    save_feature(full_df, 'PFRED_SVM')"
   ],
   "id": "4026dd8c5075d038",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==========================================\n",
    "# PART 2: COHORT ANALYSIS & PLOTTING\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from notebooks.consts import CANONICAL_GENE, CELL_LINE\n",
    "\n",
    "# Config\n",
    "INPUT_SCORED_CSV = \"data_scored_pfred_final.csv\" # Updated filename\n",
    "MIN_COHORT_SIZE = 15  # Minimum N per cohort to calculate correlation\n",
    "\n",
    "# Ensure CELL_LINE variable is defined (assuming it's a string column name like 'CELL_LINE')\n",
    "# If it's imported from consts, add it to the import above. Otherwise:\n",
    "\n",
    "if os.path.exists(INPUT_SCORED_CSV):\n",
    "    df = pd.read_csv(INPUT_SCORED_CSV, low_memory=False)\n",
    "\n",
    "\n",
    "    # Ensure columns exist and are numeric\n",
    "    target_col = 'Inhibition(%)'\n",
    "    score_cols = ['PFRED_SVM', 'PFRED_PLS'] # Updated column names\n",
    "\n",
    "    # Check if columns exist before proceeding\n",
    "    missing_cols = [c for c in score_cols + [target_col, CELL_LINE, CANONICAL_GENE] if c not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in CSV: {missing_cols}\")\n",
    "\n",
    "    # Clean data for analysis\n",
    "    for col in [target_col] + score_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Drop rows missing critical data\n",
    "    df_clean = df.dropna(subset=[target_col, CELL_LINE, CANONICAL_GENE] + score_cols)\n",
    "\n",
    "    # --- CALCULATE CORRELATIONS PER COHORT ---\n",
    "    cohort_stats = []\n",
    "\n",
    "    # Group by Cell Line and Gene\n",
    "    groups = df_clean.groupby([CELL_LINE, CANONICAL_GENE])\n",
    "\n",
    "    print(f\"Analyzing {len(groups)} potential cohorts...\")\n",
    "\n",
    "    for (cell, gene), group in groups:\n",
    "        if len(group) >= MIN_COHORT_SIZE:\n",
    "            # Calculate Correlations (Spearman for Ranking)\n",
    "            spearman_svm = group[target_col].corr(group['PFRED_SVM'], method='spearman')\n",
    "            spearman_pls = group[target_col].corr(group['PFRED_PLS'], method='spearman')\n",
    "\n",
    "            cohort_stats.append({\n",
    "                'Cohort': f\"{gene} ({cell})\",\n",
    "                'Gene': gene,\n",
    "                'Cell_Line': cell,\n",
    "                'N': len(group),\n",
    "                'Spearman_SVM': spearman_svm,\n",
    "                'Spearman_PLS': spearman_pls\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(cohort_stats)\n",
    "\n",
    "    if not results_df.empty:\n",
    "        # Sort by SVM performance\n",
    "        results_df = results_df.sort_values('Spearman_SVM', ascending=False)\n",
    "\n",
    "        print(\"\\n--- TOP 10 COHORTS (By SVM Correlation) ---\")\n",
    "        print(results_df[['Cohort', 'N', 'Spearman_SVM', 'Spearman_PLS']].head(10))\n",
    "\n",
    "        # --- PLOTTING ---\n",
    "        plt.figure(figsize=(14, 8))\n",
    "\n",
    "        # Reshape for easier plotting with Hue\n",
    "        plot_data = results_df.melt(\n",
    "            id_vars=['Cohort', 'N'],\n",
    "            value_vars=['Spearman_SVM', 'Spearman_PLS'],\n",
    "            var_name='Metric',\n",
    "            value_name='Correlation'\n",
    "        )\n",
    "\n",
    "        # Create Bar Plot\n",
    "        # Viridis is good, or 'coolwarm' to show pos/neg contrast\n",
    "        sns.barplot(data=plot_data, x='Correlation', y='Cohort', hue='Metric', palette='viridis')\n",
    "\n",
    "        plt.title(f'PFRED Correlation with Inhibition (N >= {MIN_COHORT_SIZE})')\n",
    "        plt.xlabel('Spearman Correlation (Rank Accuracy)')\n",
    "        plt.ylabel('Cohort (Gene + Cell Line)')\n",
    "        plt.axvline(0, color='black', linewidth=1)\n",
    "\n",
    "        # Add a light grid for readability\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"[WARNING] No cohorts met the minimum size requirement.\")\n",
    "else:\n",
    "    print(f\"[ERROR] Could not find {INPUT_SCORED_CSV}. Run the Mass Scoring Part 1 first.\")"
   ],
   "id": "fac799ad002d5eed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "63126ce77e385dc1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
